{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext==0.17.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-25T05:40:00.385497Z",
     "iopub.status.busy": "2025-09-25T05:40:00.385246Z",
     "iopub.status.idle": "2025-09-25T05:40:00.522646Z",
     "shell.execute_reply": "2025-09-25T05:40:00.521983Z",
     "shell.execute_reply.started": "2025-09-25T05:40:00.385480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/input/urdu-ghazals-rekhta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T05:40:03.811319Z",
     "iopub.status.busy": "2025-09-25T05:40:03.810678Z",
     "iopub.status.idle": "2025-09-25T05:40:25.092367Z",
     "shell.execute_reply": "2025-09-25T05:40:25.091540Z",
     "shell.execute_reply.started": "2025-09-25T05:40:03.811286Z"
    }
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/urdu-ghazals-rekhta/dataset /kaggle/working/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Parallel Dataset (Urdu ↔ Roman Urdu)\n",
    "\n",
    "### **In the code below, we will:**\n",
    "\n",
    "1. **Iterate through each poet’s folder** (e.g., `ahmad-faraz`, `faiz-ahmad-faiz`, etc.).  \n",
    "2. Look inside the **`ur/`** and **`en/`** subfolders.  \n",
    "3. **Read each poem file** from both subfolders.  \n",
    "4. **Collect the parallel lines**:\n",
    "   - Urdu (from `ur/`)  \n",
    "   - Roman Urdu (from `en/`)  \n",
    "5. **Save everything into two files**:\n",
    "   - `/kaggle/working/source.txt` → contains **all Urdu lines**.  \n",
    "   - `/kaggle/working/target.txt` → contains **all Roman Urdu lines**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Why is this important?**\n",
    "- Ensures poems are aligned **line-by-line** across languages.  \n",
    "- Produces a clean parallel dataset ready for **Neural Machine Translation (NMT)** training.  \n",
    "- Keeps the workflow simple: just two files (`source.txt`, `target.txt`) that will be fed into the model later.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T07:52:01.824406Z",
     "iopub.status.busy": "2025-09-25T07:52:01.824073Z",
     "iopub.status.idle": "2025-09-25T07:52:01.951938Z",
     "shell.execute_reply": "2025-09-25T07:52:01.951184Z",
     "shell.execute_reply.started": "2025-09-25T07:52:01.824385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 21003 parallel lines.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"/kaggle/working/dataset\"\n",
    "authors = os.listdir(base_dir)\n",
    "\n",
    "src_lines, tgt_lines = [], []\n",
    "\n",
    "for author in authors:\n",
    "    author_path = os.path.join(base_dir, author)\n",
    "    if not os.path.isdir(author_path):\n",
    "        continue\n",
    "    \n",
    "    ur_dir = os.path.join(author_path, \"ur\")\n",
    "    en_dir = os.path.join(author_path, \"en\")\n",
    "    \n",
    "    if not os.path.exists(ur_dir) or not os.path.exists(en_dir):\n",
    "        continue\n",
    "    \n",
    "    files = os.listdir(ur_dir)\n",
    "    for f in files:\n",
    "        ur_file = os.path.join(ur_dir, f)\n",
    "        en_file = os.path.join(en_dir, f)\n",
    "        if not os.path.exists(en_file):\n",
    "            continue\n",
    "        \n",
    "        with open(ur_file, \"r\", encoding=\"utf-8\") as fu, \\\n",
    "             open(en_file, \"r\", encoding=\"utf-8\") as fe:\n",
    "            \n",
    "            ur_lines = [l.strip() for l in fu if l.strip()]\n",
    "            en_lines = [l.strip() for l in fe if l.strip()]\n",
    "            \n",
    "            for u, e in zip(ur_lines, en_lines):\n",
    "                src_lines.append(u)\n",
    "                tgt_lines.append(e)\n",
    "\n",
    "# Save parallel corpus\n",
    "with open(\"/kaggle/working/source.txt\", \"w\", encoding=\"utf-8\") as fs, \\\n",
    "     open(\"/kaggle/working/target.txt\", \"w\", encoding=\"utf-8\") as ft:\n",
    "    for u, e in zip(src_lines, tgt_lines):\n",
    "        fs.write(u + \"\\n\")\n",
    "        ft.write(e + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(src_lines)} parallel lines.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting for Urdu ↔ Roman Urdu NMT\n",
    "\n",
    "### **What are we doing?**\n",
    "We are dividing our collected parallel dataset (**`source.txt`** for Urdu and **`target.txt`** for Roman Urdu) into **three subsets**:\n",
    "\n",
    "- **Training set (50%)** → used to fit the model.  \n",
    "- **Validation set (25%)** → used during training to tune hyperparameters and prevent overfitting.  \n",
    "- **Test set (25%)** → used **only after training** to evaluate the final model’s real performance.  \n",
    "\n",
    "---\n",
    "\n",
    "### **How is it done?**\n",
    "1. **Read** all Urdu and Roman Urdu lines.  \n",
    "2. **Shuffle** them randomly (with a fixed seed = **42** for reproducibility).  \n",
    "3. **Calculate** exact sizes (**50/25/25**).  \n",
    "4. **Write** them into **six separate files**:\n",
    "   - `train.source`, `train.target`  \n",
    "   - `val.source`, `val.target`  \n",
    "   - `test.source`, `test.target`  \n",
    "\n",
    "This ensures that Urdu and Roman Urdu lines remain **aligned**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is this important?**\n",
    "- **NMT models** require separate **train, validation, and test** splits to evaluate generalization.  \n",
    "- **50/25/25 split** provides a good balance → enough data to train while still having reliable evaluation sets.  \n",
    "- **Reproducibility** (fixed seed) ensures results can be compared consistently across experiments.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T07:52:06.677083Z",
     "iopub.status.busy": "2025-09-25T07:52:06.676818Z",
     "iopub.status.idle": "2025-09-25T07:52:06.735961Z",
     "shell.execute_reply": "2025-09-25T07:52:06.735198Z",
     "shell.execute_reply.started": "2025-09-25T07:52:06.677063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 10501 lines to train.source and train.target\n",
      "Wrote 5250 lines to val.source and val.target\n",
      "Wrote 5252 lines to test.source and test.target\n",
      "Total lines: 21003  -> train: 10501, val: 5250, test: 5252\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into train/val/test (50/25/25) with reproducible shuffle\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "base = Path(\"/kaggle/working\")\n",
    "src_path = base / \"source.txt\"\n",
    "tgt_path = base / \"target.txt\"\n",
    "\n",
    "# read\n",
    "with src_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    src_lines = [l.rstrip(\"\\n\") for l in f]\n",
    "with tgt_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    tgt_lines = [l.rstrip(\"\\n\") for l in f]\n",
    "\n",
    "assert len(src_lines) == len(tgt_lines), \"Source and target line counts differ!\"\n",
    "\n",
    "n = len(src_lines)\n",
    "indices = list(range(n))\n",
    "random.shuffle(indices)\n",
    "\n",
    "def write_split(name, idxs):\n",
    "    out_src = base / f\"{name}.source\"\n",
    "    out_tgt = base / f\"{name}.target\"\n",
    "    with out_src.open(\"w\", encoding=\"utf-8\") as fs, out_tgt.open(\"w\", encoding=\"utf-8\") as ft:\n",
    "        for i in idxs:\n",
    "            fs.write(src_lines[i] + \"\\n\")\n",
    "            ft.write(tgt_lines[i] + \"\\n\")\n",
    "    print(f\"Wrote {len(idxs)} lines to {out_src.name} and {out_tgt.name}\")\n",
    "\n",
    "# sizes\n",
    "train_n = int(0.50 * n)\n",
    "val_n   = int(0.25 * n)\n",
    "test_n  = n - train_n - val_n\n",
    "\n",
    "train_idx = indices[:train_n]\n",
    "val_idx   = indices[train_n:train_n+val_n]\n",
    "test_idx  = indices[train_n+val_n:]\n",
    "\n",
    "write_split(\"train\", train_idx)\n",
    "write_split(\"val\", val_idx)\n",
    "write_split(\"test\", test_idx)\n",
    "\n",
    "print(f\"Total lines: {n}  -> train: {len(train_idx)}, val: {len(val_idx)}, test: {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Normalizing Urdu ↔ Roman Urdu Dataset\n",
    "\n",
    "We normalize the **source (Urdu)** and **target (Roman Urdu)** lines for each **data split** and write the cleaned files to:\n",
    "\n",
    "/kaggle/working/normalized/{train,val,test}.source\n",
    "\n",
    "/kaggle/working/normalized/{train,val,test}.target\n",
    "\n",
    "### **🔧 How**\n",
    "\n",
    "#### **Urdu normalization (`normalize_urdu`)**\n",
    "- Remove **tatweel (ـ)** and optional **diacritics (tashkeel)** to reduce unseen variants.  \n",
    "-  Normalize multiple orthographic variants into single forms:  \n",
    "  - `إ أ آ ٱ → ا`  \n",
    "  - `ي → ی`  \n",
    "  - `ك → ک`  \n",
    "- Replace common **presentation ligatures**.  \n",
    "- Collapse multiple whitespace.  \n",
    "- *Optional*: Remove punctuation while preserving Urdu-specific marks like `،` `۔` `؟`.  \n",
    "\n",
    "#### **Roman normalization (`normalize_roman`)**\n",
    "- Strip leading/trailing whitespace.  \n",
    "- Convert to lowercase.  \n",
    "- Collapse repeated spaces.  \n",
    "- *Optional*: Remove punctuation, but **keep diacritic letters** (e.g., `ā`, `ī`) since they are informative for transliteration.  \n",
    "\n",
    "---\n",
    "\n",
    "### **❓ Why**\n",
    "\n",
    "- **Poetry** often contains stylistic marks + inconsistent spellings → this increases vocabulary sparsity and harms model learning.  \n",
    "- **Normalization reduces irrelevant variation** so the model can focus on phonetic/orthographic mapping.  \n",
    "- We **do not aggressively strip punctuation by default** because punctuation in poetry encodes structure (pauses, line breaks).  \n",
    "  - You can enable `remove_extra_punct=True` for a stricter dataset (e.g., ablation experiments).  \n",
    "- Keeping **diacritics in Roman target** helps the model learn precise **phonetic correspondences** → useful for **transliteration tasks**.  \n",
    "-  If plain ASCII Roman Urdu is required, we can later **remove or map diacritics**.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T07:52:15.081084Z",
     "iopub.status.busy": "2025-09-25T07:52:15.080818Z",
     "iopub.status.idle": "2025-09-25T07:52:15.089087Z",
     "shell.execute_reply": "2025-09-25T07:52:15.088539Z",
     "shell.execute_reply.started": "2025-09-25T07:52:15.081064Z"
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced normalization with quality filtering\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "def normalize_urdu(text, remove_tashkeel=True, remove_tatweel=True,\n",
    "                  normalize_alef=True, normalize_ye=True, normalize_kaf=True,\n",
    "                  remove_extra_punct=False):\n",
    "    \"\"\"Enhanced Urdu normalization with better error handling\"\"\"\n",
    "    if text is None or not text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    s = text.strip()\n",
    "    \n",
    "    # Remove tatweel (kashida)\n",
    "    if remove_tatweel:\n",
    "        s = re.sub('\\u0640+', '', s)\n",
    "    \n",
    "    # Remove tashkeel / diacritics / vowel marks (optional)\n",
    "    if remove_tashkeel:\n",
    "        s = re.sub(r'[\\u0610-\\u061A\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]', '', s)\n",
    "    \n",
    "    # Normalize alef variants to bare alef\n",
    "    if normalize_alef:\n",
    "        s = re.sub('[إأآٱ]', 'ا', s)\n",
    "    \n",
    "    # Normalize Arabic Yeh (ي) to Persian/Urdu Yeh (ی)\n",
    "    if normalize_ye:\n",
    "        s = s.replace('ي', 'ی')\n",
    "    \n",
    "    # Normalize Arabic Kaf (ك) to Persian/Urdu Kaf (ک)\n",
    "    if normalize_kaf:\n",
    "        s = s.replace('ك', 'ک')\n",
    "    \n",
    "    # Replace some presentation-form ligatures if present\n",
    "    s = s.replace('ﻻ', 'لا')\n",
    "    \n",
    "    # Collapse multiple spaces/newlines\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    \n",
    "    # Optionally remove most punctuation while keeping Urdu punctuation\n",
    "    if remove_extra_punct:\n",
    "        keep = set(['،', '۔', '؟'])\n",
    "        s = ''.join(ch for ch in s if (unicodedata.category(ch)[0] != 'P') or (ch in keep))\n",
    "        s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "def normalize_roman(text, lower=True, remove_extra_punct=False):\n",
    "    \"\"\"Enhanced Roman Urdu normalization\"\"\"\n",
    "    if text is None or not text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    s = text.strip()\n",
    "    \n",
    "    if lower:\n",
    "        s = s.lower()\n",
    "    \n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    \n",
    "    if remove_extra_punct:\n",
    "        s = ''.join(ch for ch in s if unicodedata.category(ch)[0] != 'P')\n",
    "        s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T07:52:22.157128Z",
     "iopub.status.busy": "2025-09-25T07:52:22.156459Z",
     "iopub.status.idle": "2025-09-25T07:52:22.672044Z",
     "shell.execute_reply": "2025-09-25T07:52:22.671437Z",
     "shell.execute_reply.started": "2025-09-25T07:52:22.157101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting enhanced normalization with quality filtering...\n",
      "\n",
      "TRAIN: 10501 -> 10445 (56 filtered, 0.5%)\n",
      "\n",
      "VAL: 5250 -> 5219 (31 filtered, 0.6%)\n",
      "\n",
      "TEST: 5252 -> 5223 (29 filtered, 0.6%)\n",
      "\n",
      "=== SUMMARY ===\n",
      "Total pairs filtered out: 116\n",
      "\n",
      "--- TRAIN SAMPLES (10445 pairs) ---\n",
      "1) URDU:  جو ازل سے چھڑ گیا ہے اس فسانے کی کہو\n",
      "   ROMAN: jo azal se chhiḍ gayā hai us fasāne kī kaho\n",
      "   WORDS: 10 -> 10\n",
      "2) URDU:  ایک بے چہرہ سی امید ہے چہرہ چہرہ\n",
      "   ROMAN: ek be-chehra sī ummīd hai chehra chehra\n",
      "   WORDS: 8 -> 7\n",
      "3) URDU:  حسن کے خضر نے کیا لبریز\n",
      "   ROMAN: husn ke ḳhizr ne kiyā labrez\n",
      "   WORDS: 6 -> 6\n",
      "4) URDU:  برسا بھی تو کس دشت کے بے فیض بدن پر\n",
      "   ROMAN: barsā bhī to kis dasht ke be-faiz badan par\n",
      "   WORDS: 10 -> 9\n",
      "5) URDU:  کچھ دور چل کے راستے سب ایک سے لگے\n",
      "   ROMAN: kuchh duur chal ke rāste sab ek se lage\n",
      "   WORDS: 9 -> 9\n",
      "\n",
      "--- VAL SAMPLES (5219 pairs) ---\n",
      "1) URDU:  استان یار سے اٹھ جائیں کیا\n",
      "   ROMAN: āstān-e-yār se uth jaa.eñ kyā\n",
      "   WORDS: 6 -> 5\n",
      "2) URDU:  اتا ہے ہوش مجھ کو اب تو پہر پہر میں\n",
      "   ROMAN: aatā hai hosh mujh ko ab to pahr pahr meñ\n",
      "   WORDS: 10 -> 10\n",
      "3) URDU:  صبح کے درد کو راتوں کی جلن کو بھولیں\n",
      "   ROMAN: sub.h ke dard ko rātoñ kī jalan ko bhūleñ\n",
      "   WORDS: 9 -> 9\n",
      "4) URDU:  یہ ہمیں تھے جن کے لباس پر سر رہ سیاہی لکھی گئی\n",
      "   ROMAN: ye hamīñ the jin ke libās par sar-e-rah siyāhī likhī ga.ī\n",
      "   WORDS: 12 -> 11\n",
      "5) URDU:  خدا کی خدائی میں تجھ سا نہ دیکھا\n",
      "   ROMAN: ḳhudā kī ḳhudā.ī meñ tujh sā na dekhā\n",
      "   WORDS: 8 -> 8\n",
      "\n",
      "--- TEST SAMPLES (5223 pairs) ---\n",
      "1) URDU:  ہیرے کے نورتن نہیں تیرے ہوئے ہیں جما\n",
      "   ROMAN: hiire ke nau-ratan nahīñ tere hue haiñ jamā\n",
      "   WORDS: 8 -> 8\n",
      "2) URDU:  تری طرف ہے بہ حسرت نظارۂ نرگس\n",
      "   ROMAN: tirī taraf hai ba-hasrat nazāra-e-nargis\n",
      "   WORDS: 7 -> 5\n",
      "3) URDU:  صد حیف وہ ناکام کہ اک عمر سے غالب\n",
      "   ROMAN: sad-haif vo nākām ki ik umr se 'ġhālib'\n",
      "   WORDS: 9 -> 8\n",
      "4) URDU:  سبزہ و گل کے دیکھنے کے لیے\n",
      "   ROMAN: sabza o gul ke dekhne ke liye\n",
      "   WORDS: 7 -> 7\n",
      "5) URDU:  اشک خوناب ہو چلے ہیں\n",
      "   ROMAN: ashk ḳhūnāb ho chale haiñ\n",
      "   WORDS: 5 -> 5\n",
      "\n",
      "=== QUALITY ANALYSIS ===\n",
      "Source word count - Min: 3, Max: 25, Avg: 8.2\n",
      "Target word count - Min: 2, Max: 23, Avg: 7.5\n",
      "Estimated source vocabulary: 10,207\n",
      "Estimated target vocabulary: 17,161\n",
      "\n",
      "✅ Enhanced normalization completed!\n"
     ]
    }
   ],
   "source": [
    "def is_valid_pair(urdu_text, roman_text, min_words=2, max_words=50, min_chars=5, max_chars=200):\n",
    "    \"\"\"Quality filtering for sentence pairs\"\"\"\n",
    "    # Check if either is empty after normalization\n",
    "    if not urdu_text.strip() or not roman_text.strip():\n",
    "        return False\n",
    "    \n",
    "    # Word count filtering\n",
    "    urdu_words = len(urdu_text.split())\n",
    "    roman_words = len(roman_text.split())\n",
    "    \n",
    "    if urdu_words < min_words or roman_words < min_words:\n",
    "        return False\n",
    "    if urdu_words > max_words or roman_words > max_words:\n",
    "        return False\n",
    "    \n",
    "    # Character count filtering  \n",
    "    if len(urdu_text) < min_chars or len(roman_text) < min_chars:\n",
    "        return False\n",
    "    if len(urdu_text) > max_chars or len(roman_text) > max_chars:\n",
    "        return False\n",
    "    \n",
    "    # Length ratio check (avoid severely misaligned pairs)\n",
    "    word_ratio = max(urdu_words, roman_words) / min(urdu_words, roman_words)\n",
    "    if word_ratio > 2.5:  # One sentence is >2.5x longer than the other\n",
    "        return False\n",
    "    \n",
    "    # Check if sentences are identical (likely error)\n",
    "    if urdu_text == roman_text:\n",
    "        return False\n",
    "    \n",
    "    # Check for reasonable character diversity\n",
    "    urdu_unique_chars = len(set(urdu_text))\n",
    "    roman_unique_chars = len(set(roman_text))\n",
    "    \n",
    "    if urdu_unique_chars < 3 or roman_unique_chars < 3:  # Too repetitive\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "# Enhanced normalization with quality filtering\n",
    "base = Path(\"/kaggle/working\")\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "out_dir = base / \"normalized\"\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "summary = {}\n",
    "total_filtered = 0\n",
    "\n",
    "print(\"Starting enhanced normalization with quality filtering...\")\n",
    "\n",
    "for sp in splits:\n",
    "    src_in = base / f\"{sp}.source\"\n",
    "    tgt_in = base / f\"{sp}.target\"\n",
    "    src_out = out_dir / f\"{sp}.source\"\n",
    "    tgt_out = out_dir / f\"{sp}.target\"\n",
    "    \n",
    "    if not src_in.exists() or not tgt_in.exists():\n",
    "        print(f\"Skipping {sp}: missing {src_in.name} or {tgt_in.name}\")\n",
    "        continue\n",
    "    \n",
    "    with src_in.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        src_lines = [l.rstrip(\"\\n\") for l in f]\n",
    "    with tgt_in.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        tgt_lines = [l.rstrip(\"\\n\") for l in f]\n",
    "    \n",
    "    assert len(src_lines) == len(tgt_lines), f\"Line count mismatch in {sp}\"\n",
    "    \n",
    "    # Normalize and filter\n",
    "    norm_src, norm_tgt = [], []\n",
    "    filtered_count = 0\n",
    "    \n",
    "    for i, (src, tgt) in enumerate(zip(src_lines, tgt_lines)):\n",
    "        # Normalize\n",
    "        norm_s = normalize_urdu(src, remove_tashkeel=True, remove_tatweel=True,\n",
    "                               normalize_alef=True, normalize_ye=True, normalize_kaf=True,\n",
    "                               remove_extra_punct=False)\n",
    "        norm_t = normalize_roman(tgt, lower=True, remove_extra_punct=False)\n",
    "        \n",
    "        # Quality filter\n",
    "        if is_valid_pair(norm_s, norm_t):\n",
    "            norm_src.append(norm_s)\n",
    "            norm_tgt.append(norm_t)\n",
    "        else:\n",
    "            filtered_count += 1\n",
    "    \n",
    "    # Save filtered, normalized files\n",
    "    with src_out.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(norm_src))\n",
    "    with tgt_out.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(norm_tgt))\n",
    "    \n",
    "    # Statistics\n",
    "    original_count = len(src_lines)\n",
    "    final_count = len(norm_src)\n",
    "    filter_rate = (filtered_count / original_count) * 100\n",
    "    \n",
    "    print(f\"\\n{sp.upper()}: {original_count} -> {final_count} ({filtered_count} filtered, {filter_rate:.1f}%)\")\n",
    "    \n",
    "    # Sample pairs for verification\n",
    "    sample_pairs = list(zip(norm_src[:5], norm_tgt[:5]))\n",
    "    summary[sp] = {\n",
    "        \"original\": original_count,\n",
    "        \"final\": final_count, \n",
    "        \"filtered\": filtered_count,\n",
    "        \"sample\": sample_pairs\n",
    "    }\n",
    "    \n",
    "    total_filtered += filtered_count\n",
    "\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "print(f\"Total pairs filtered out: {total_filtered}\")\n",
    "\n",
    "# Print samples\n",
    "for sp, info in summary.items():\n",
    "    print(f\"\\n--- {sp.upper()} SAMPLES ({info['final']} pairs) ---\")\n",
    "    for i, (u, r) in enumerate(info[\"sample\"], 1):\n",
    "        print(f\"{i}) URDU:  {u}\")\n",
    "        print(f\"   ROMAN: {r}\")\n",
    "        print(f\"   WORDS: {len(u.split())} -> {len(r.split())}\")\n",
    "\n",
    "# Additional quality checks\n",
    "print(f\"\\n=== QUALITY ANALYSIS ===\")\n",
    "all_src = []\n",
    "all_tgt = []\n",
    "\n",
    "for sp in [\"train\", \"val\", \"test\"]:\n",
    "    src_file = out_dir / f\"{sp}.source\"\n",
    "    tgt_file = out_dir / f\"{sp}.target\"\n",
    "    \n",
    "    if src_file.exists() and tgt_file.exists():\n",
    "        with src_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            all_src.extend([l.strip() for l in f if l.strip()])\n",
    "        with tgt_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            all_tgt.extend([l.strip() for l in f if l.strip()])\n",
    "\n",
    "if all_src and all_tgt:\n",
    "    src_lengths = [len(s.split()) for s in all_src]\n",
    "    tgt_lengths = [len(t.split()) for t in all_tgt]\n",
    "    \n",
    "    print(f\"Source word count - Min: {min(src_lengths)}, Max: {max(src_lengths)}, Avg: {sum(src_lengths)/len(src_lengths):.1f}\")\n",
    "    print(f\"Target word count - Min: {min(tgt_lengths)}, Max: {max(tgt_lengths)}, Avg: {sum(tgt_lengths)/len(tgt_lengths):.1f}\")\n",
    "    \n",
    "    # Vocabulary size estimation\n",
    "    src_vocab = set(' '.join(all_src).split())\n",
    "    tgt_vocab = set(' '.join(all_tgt).split())\n",
    "    \n",
    "    print(f\"Estimated source vocabulary: {len(src_vocab):,}\")\n",
    "    print(f\"Estimated target vocabulary: {len(tgt_vocab):,}\")\n",
    "\n",
    "print(\"\\n✅ Enhanced normalization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePiece Tokenization for Urdu → Roman Urdu\n",
    "\n",
    "---\n",
    "\n",
    "### What we are doing\n",
    "We will **train a SentencePiece tokenizer** on our **Urdu → Roman Urdu dataset** and then apply it to all splits (**train, validation, test**).  \n",
    "This will convert raw text into **token IDs** that can be fed into the **BiLSTM seq2seq model**.  \n",
    "\n",
    "---\n",
    "\n",
    "### How we are doing it\n",
    "1.  **Train two SentencePiece models**:\n",
    "   - One for **Urdu (source language)**.  \n",
    "   - One for **Roman Urdu (target language)**.  \n",
    "2.  Save the trained models (`.model`, `.vocab`).  \n",
    "3.  Encode dataset files (`train.source`, `train.target`, `val.*`, `test.*`) into **integer token IDs**.  \n",
    "\n",
    "---\n",
    "\n",
    "### Why we are doing this\n",
    "- Neural networks work with **numbers, not raw text**.  \n",
    "- **Tokenization bridges this gap** by mapping text → tokens → IDs.  \n",
    "\n",
    "---\n",
    "\n",
    "### Why SentencePiece over others?\n",
    "\n",
    "| Method | Problem |\n",
    "|--------|---------|\n",
    "| **One-hot encoding** |  Inefficient, huge sparse vectors, no subword knowledge. |\n",
    "| **Word-level tokenization** |  Breaks on unseen/rare poetic words in Urdu. |\n",
    "| **Character-level tokenization** |  Handles everything, but creates long sequences & loses semantics. |\n",
    "| **Subword (SentencePiece / BPE)** |  Handles rare & unseen words.<br> Keeps vocabulary compact.<br> Learns meaningful chunks (e.g., *mohabbat* → `[\"moh\", \"abbat\"]`). |\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**  \n",
    "**SentencePiece** is **ideal** for our low-resource dataset (**Urdu ghazals**) because it generalizes better than word-level or character-level methods, while still keeping vocabulary size manageable.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T07:53:05.375359Z",
     "iopub.status.busy": "2025-09-25T07:53:05.374574Z",
     "iopub.status.idle": "2025-09-25T07:53:07.407011Z",
     "shell.execute_reply": "2025-09-25T07:53:07.406389Z",
     "shell.execute_reply.started": "2025-09-25T07:53:05.375334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SentencePiece models...\n",
      "✅ SentencePiece models trained and saved!\n",
      "Urdu vocab size: 8000\n",
      "Roman vocab size: 8000\n",
      "Urdu - UNK: 0, PAD: 1, BOS: 2, EOS: 3\n",
      "Roman - UNK: 0, PAD: 1, BOS: 2, EOS: 3\n",
      "\n",
      "=== TOKENIZATION QUALITY TEST ===\n",
      "\n",
      "Test 1:\n",
      "Urdu Original:  میں تمہیں بہت پسند کرتا ہوں\n",
      "Urdu Tokens:    ['▁میں', '▁تمہیں', '▁بہت', '▁پسند', '▁کرتا', '▁ہوں']\n",
      "Urdu IDs:       [20, 576, 179, 2073, 959, 92]\n",
      "Urdu Decoded:   میں تمہیں بہت پسند کرتا ہوں\n",
      "Roman Original: maiñ tumheñ bahut pasand kartā hūñ\n",
      "Roman Tokens:   ['▁maiñ', '▁tumheñ', '▁bahut', '▁pasand', '▁kartā', '▁h', 'ūñ']\n",
      "Roman IDs:      [112, 811, 239, 4185, 1108, 18, 121]\n",
      "Roman Decoded:  maiñ tumheñ bahut pasand kartā hūñ\n",
      "\n",
      "Test 2:\n",
      "Urdu Original:  یہ ایک خوبصورت دن ہے\n",
      "Urdu Tokens:    ['▁یہ', '▁ایک', '▁خوب', 'صورت', '▁دن', '▁ہے']\n",
      "Urdu IDs:       [59, 170, 1066, 6512, 151, 15]\n",
      "Urdu Decoded:   یہ ایک خوبصورت دن ہے\n",
      "Roman Original: ye aik ḳhūbsūrat din hai\n",
      "Roman Tokens:   ['▁ye', '▁a', 'ik', '▁ḳhūb', 'sūrat', '▁din', '▁hai']\n",
      "Roman IDs:      [86, 15, 76, 3116, 3067, 311, 11]\n",
      "Roman Decoded:  ye aik ḳhūbsūrat din hai\n",
      "\n",
      "Test 3:\n",
      "Urdu Original:  شاعری کی دنیا بہت وسیع ہے\n",
      "Urdu Tokens:    ['▁شاعری', '▁کی', '▁دنیا', '▁بہت', '▁وسیع', '▁ہے']\n",
      "Urdu IDs:       [2096, 28, 353, 179, 5016, 15]\n",
      "Urdu Decoded:   شاعری کی دنیا بہت وسیع ہے\n",
      "Roman Original: shā.irī kī duniyā bahut vasī.a hai\n",
      "Roman Tokens:   ['▁shā', '.', 'irī', '▁kī', '▁duniyā', '▁bahut', '▁vasī', '.', 'a', '▁hai']\n",
      "Roman IDs:      [245, 7988, 2943, 43, 468, 239, 3981, 7988, 7962, 11]\n",
      "Roman Decoded:  shā.irī kī duniyā bahut vasī.a hai\n",
      "\n",
      "=== ENCODING DATASETS ===\n",
      "Processing train split...\n",
      "  Encoded 10445 sentences to train.src\n",
      "  Encoded 10445 sentences to train.tgt\n",
      "Processing val split...\n",
      "  Encoded 5219 sentences to val.src\n",
      "  Encoded 5219 sentences to val.tgt\n",
      "Processing test split...\n",
      "  Encoded 5223 sentences to test.src\n",
      "  Encoded 5223 sentences to test.tgt\n",
      "✅ All dataset splits tokenized with BOS/EOS tokens!\n",
      "\n",
      "=== TOKENIZED DATA VALIDATION ===\n",
      "TRAIN: 10445 src, 10445 tgt\n",
      "  Sample src: 2 69 1874 27 7030 126 15 42 3522 28 854 3\n",
      "  Sample tgt: 2 94 3075 27 4042 164 11 91 4149 43 961 3\n",
      "  Decoded src: جو ازل سے چھڑ گیا ہے اس فسانے کی کہو\n",
      "  Decoded tgt: jo azal se chhiḍ gayā hai us fasāne kī kaho\n",
      "\n",
      "VAL: 5219 src, 5219 tgt\n",
      "  Sample src: 2 6357 258 27 407 491 51 3\n",
      "  Sample tgt: 2 452 1700 7974 7964 7974 416 27 635 88 7988 21 79 3\n",
      "  Decoded src: استان یار سے اٹھ جائیں کیا\n",
      "  Decoded tgt: āstān-e-yār se uth jaa.eñ kyā\n",
      "\n",
      "TEST: 5223 src, 5223 tgt\n",
      "  Sample src: 2 6294 35 12 435 7960 54 378 266 47 6919 3\n",
      "  Sample tgt: 2 5740 7964 45 1810 7974 284 71 66 446 309 59 5747 3\n",
      "  Decoded src: ہیرے کے نورتن نہیں تیرے ہوئے ہیں جما\n",
      "  Decoded tgt: hiire ke nau-ratan nahīñ tere hue haiñ jamā\n",
      "\n",
      "=== TOKENIZATION STATISTICS ===\n",
      "Source token lengths - Min: 5, Max: 27, Avg: 10.6\n",
      "Target token lengths - Min: 5, Max: 36, Avg: 12.9\n",
      "\n",
      "✅ Improved tokenization completed!\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "# Paths\n",
    "# ----------------------------\n",
    "data_dir = \"/kaggle/working/normalized\"   # use normalized data\n",
    "smp_dir  = \"/kaggle/working/spm_models\"   # save trained models\n",
    "tok_dir  = \"/kaggle/working/tokenized\"    # save tokenized data\n",
    "\n",
    "os.makedirs(smp_dir, exist_ok=True)\n",
    "os.makedirs(tok_dir, exist_ok=True)\n",
    "\n",
    "train_src = os.path.join(data_dir, \"train.source\")\n",
    "train_tgt = os.path.join(data_dir, \"train.target\")\n",
    "\n",
    "print(\"Training SentencePiece models...\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Train SentencePiece Models\n",
    "# ----------------------------\n",
    "# Urdu (source)\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=train_src,\n",
    "    model_prefix=os.path.join(smp_dir, \"spm_ur\"),\n",
    "    vocab_size=8000,  # Increased from 5000 for better coverage\n",
    "    character_coverage=1.0,   # cover full Urdu script\n",
    "    model_type=\"bpe\",\n",
    "    unk_id=0, pad_id=1, bos_id=2, eos_id=3,\n",
    "    max_sentence_length=512,  # Add length limit\n",
    "    shuffle_input_sentence=True,  # Better training\n",
    "    split_by_whitespace=True\n",
    ")\n",
    "\n",
    "# Roman Urdu (target)\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=train_tgt,\n",
    "    model_prefix=os.path.join(smp_dir, \"spm_ro\"),\n",
    "    vocab_size=8000,  # Increased from 5000 for better coverage\n",
    "    character_coverage=1.0,\n",
    "    model_type=\"bpe\",\n",
    "    unk_id=0, pad_id=1, bos_id=2, eos_id=3,\n",
    "    max_sentence_length=512,  # Add length limit\n",
    "    shuffle_input_sentence=True,  # Better training\n",
    "    split_by_whitespace=True\n",
    ")\n",
    "\n",
    "print(\"✅ SentencePiece models trained and saved!\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Load Tokenizers and Validate\n",
    "# ----------------------------\n",
    "sp_ur = spm.SentencePieceProcessor(model_file=os.path.join(smp_dir, \"spm_ur.model\"))\n",
    "sp_ro = spm.SentencePieceProcessor(model_file=os.path.join(smp_dir, \"spm_ro.model\"))\n",
    "\n",
    "print(f\"Urdu vocab size: {sp_ur.get_piece_size()}\")\n",
    "print(f\"Roman vocab size: {sp_ro.get_piece_size()}\")\n",
    "\n",
    "# Validate special tokens\n",
    "print(f\"Urdu - UNK: {sp_ur.unk_id()}, PAD: {sp_ur.pad_id()}, BOS: {sp_ur.bos_id()}, EOS: {sp_ur.eos_id()}\")\n",
    "print(f\"Roman - UNK: {sp_ro.unk_id()}, PAD: {sp_ro.pad_id()}, BOS: {sp_ro.bos_id()}, EOS: {sp_ro.eos_id()}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Test Tokenization Quality\n",
    "# ----------------------------\n",
    "print(\"\\n=== TOKENIZATION QUALITY TEST ===\")\n",
    "\n",
    "test_sentences_ur = [\n",
    "    \"میں تمہیں بہت پسند کرتا ہوں\",\n",
    "    \"یہ ایک خوبصورت دن ہے\",\n",
    "    \"شاعری کی دنیا بہت وسیع ہے\"\n",
    "]\n",
    "\n",
    "test_sentences_ro = [\n",
    "    \"maiñ tumheñ bahut pasand kartā hūñ\",\n",
    "    \"ye aik ḳhūbsūrat din hai\", \n",
    "    \"shā.irī kī duniyā bahut vasī.a hai\"\n",
    "]\n",
    "\n",
    "for i, (ur_sent, ro_sent) in enumerate(zip(test_sentences_ur, test_sentences_ro), 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    \n",
    "    # Urdu tokenization\n",
    "    ur_tokens = sp_ur.encode(ur_sent, out_type=str)\n",
    "    ur_ids = sp_ur.encode(ur_sent, out_type=int)\n",
    "    ur_decoded = sp_ur.decode(ur_ids)\n",
    "    \n",
    "    print(f\"Urdu Original:  {ur_sent}\")\n",
    "    print(f\"Urdu Tokens:    {ur_tokens}\")\n",
    "    print(f\"Urdu IDs:       {ur_ids}\")\n",
    "    print(f\"Urdu Decoded:   {ur_decoded}\")\n",
    "    \n",
    "    # Roman tokenization\n",
    "    ro_tokens = sp_ro.encode(ro_sent, out_type=str)\n",
    "    ro_ids = sp_ro.encode(ro_sent, out_type=int)\n",
    "    ro_decoded = sp_ro.decode(ro_ids)\n",
    "    \n",
    "    print(f\"Roman Original: {ro_sent}\")\n",
    "    print(f\"Roman Tokens:   {ro_tokens}\")\n",
    "    print(f\"Roman IDs:      {ro_ids}\")\n",
    "    print(f\"Roman Decoded:  {ro_decoded}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4. IMPROVED Encoding with BOS/EOS tokens\n",
    "# ----------------------------\n",
    "def encode_file_with_special_tokens(input_path, output_path, sp, add_bos=True, add_eos=True):\n",
    "    \"\"\"Encode file with proper BOS/EOS token handling\"\"\"\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"Warning: {input_path} does not exist!\")\n",
    "        return\n",
    "    \n",
    "    encoded_count = 0\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        \n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Encode the sentence\n",
    "            ids = sp.encode(line, out_type=int)\n",
    "            \n",
    "            # Add special tokens\n",
    "            if add_bos:\n",
    "                ids = [sp.bos_id()] + ids\n",
    "            if add_eos:\n",
    "                ids = ids + [sp.eos_id()]\n",
    "            \n",
    "            # Write to file\n",
    "            fout.write(\" \".join(map(str, ids)) + \"\\n\")\n",
    "            encoded_count += 1\n",
    "    \n",
    "    print(f\"  Encoded {encoded_count} sentences to {os.path.basename(output_path)}\")\n",
    "\n",
    "print(\"\\n=== ENCODING DATASETS ===\")\n",
    "\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "for split in splits:\n",
    "    print(f\"Processing {split} split...\")\n",
    "    \n",
    "    # Source (Urdu) - add BOS/EOS for encoder input\n",
    "    src_input = f\"{data_dir}/{split}.source\"\n",
    "    src_output = f\"{tok_dir}/{split}.src\"\n",
    "    encode_file_with_special_tokens(src_input, src_output, sp_ur, add_bos=True, add_eos=True)\n",
    "    \n",
    "    # Target (Roman) - add BOS/EOS for decoder\n",
    "    tgt_input = f\"{data_dir}/{split}.target\"\n",
    "    tgt_output = f\"{tok_dir}/{split}.tgt\"\n",
    "    encode_file_with_special_tokens(tgt_input, tgt_output, sp_ro, add_bos=True, add_eos=True)\n",
    "\n",
    "print(\"✅ All dataset splits tokenized with BOS/EOS tokens!\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Validate Tokenized Data\n",
    "# ----------------------------\n",
    "print(\"\\n=== TOKENIZED DATA VALIDATION ===\")\n",
    "\n",
    "for split in splits:\n",
    "    src_file = f\"{tok_dir}/{split}.src\"\n",
    "    tgt_file = f\"{tok_dir}/{split}.tgt\"\n",
    "    \n",
    "    if os.path.exists(src_file) and os.path.exists(tgt_file):\n",
    "        with open(src_file, \"r\") as f_src, open(tgt_file, \"r\") as f_tgt:\n",
    "            src_lines = [l.strip() for l in f_src if l.strip()]\n",
    "            tgt_lines = [l.strip() for l in f_tgt if l.strip()]\n",
    "        \n",
    "        print(f\"{split.upper()}: {len(src_lines)} src, {len(tgt_lines)} tgt\")\n",
    "        \n",
    "        if len(src_lines) != len(tgt_lines):\n",
    "            print(f\"  ⚠️  WARNING: Mismatched counts in {split}!\")\n",
    "        \n",
    "        # Show sample tokenized pairs\n",
    "        if src_lines and tgt_lines:\n",
    "            print(f\"  Sample src: {src_lines[0]}\")\n",
    "            print(f\"  Sample tgt: {tgt_lines[0]}\")\n",
    "            \n",
    "            # Decode to verify\n",
    "            src_ids = list(map(int, src_lines[0].split()))\n",
    "            tgt_ids = list(map(int, tgt_lines[0].split()))\n",
    "            \n",
    "            src_decoded = sp_ur.decode(src_ids)\n",
    "            tgt_decoded = sp_ro.decode(tgt_ids)\n",
    "            \n",
    "            print(f\"  Decoded src: {src_decoded}\")\n",
    "            print(f\"  Decoded tgt: {tgt_decoded}\")\n",
    "            print()\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Generate Statistics\n",
    "# ----------------------------\n",
    "print(\"=== TOKENIZATION STATISTICS ===\")\n",
    "\n",
    "all_src_lengths = []\n",
    "all_tgt_lengths = []\n",
    "\n",
    "for split in splits:\n",
    "    src_file = f\"{tok_dir}/{split}.src\"\n",
    "    if os.path.exists(src_file):\n",
    "        with open(src_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    length = len(line.strip().split())\n",
    "                    all_src_lengths.append(length)\n",
    "\n",
    "for split in splits:\n",
    "    tgt_file = f\"{tok_dir}/{split}.tgt\"\n",
    "    if os.path.exists(tgt_file):\n",
    "        with open(tgt_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    length = len(line.strip().split())\n",
    "                    all_tgt_lengths.append(length)\n",
    "\n",
    "if all_src_lengths and all_tgt_lengths:\n",
    "    print(f\"Source token lengths - Min: {min(all_src_lengths)}, Max: {max(all_src_lengths)}, Avg: {sum(all_src_lengths)/len(all_src_lengths):.1f}\")\n",
    "    print(f\"Target token lengths - Min: {min(all_tgt_lengths)}, Max: {max(all_tgt_lengths)}, Avg: {sum(all_tgt_lengths)/len(all_tgt_lengths):.1f}\")\n",
    "\n",
    "print(\"\\n✅ Improved tokenization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & DataLoader in PyTorch\n",
    "\n",
    "---\n",
    "\n",
    "### What we are doing\n",
    "We need to prepare our tokenized data (`train.source`, `train.target`, etc.) so the BiLSTM model can use it.  \n",
    "\n",
    "- PyTorch requires data in batches of padded sequences.  \n",
    "- The SentencePiece tokenizer gives us IDs for each token.  \n",
    "- We will build a custom Dataset class and use DataLoader to efficiently feed data during training.  \n",
    "\n",
    "---\n",
    "\n",
    "### How we do it\n",
    "1. Load the SentencePiece models (`urdu.model` and `roman.model`).  \n",
    "2. Read the tokenized files (train/val/test splits).  \n",
    "3. Convert each line into integer token IDs using the tokenizer.  \n",
    "4. Pad sequences so that all examples in a batch have the same length.  \n",
    "5. Wrap everything inside a PyTorch Dataset and feed with DataLoader.  \n",
    "\n",
    "---\n",
    "\n",
    "### Why we do it\n",
    "- Neural networks need fixed-size tensors, but sentences naturally have variable lengths.  \n",
    "- Padding and batching allow the model to process multiple sentences at once, making training much faster.  \n",
    "- A clean Dataset class keeps the code modular, reusable, and easy to debug.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T07:53:20.556015Z",
     "iopub.status.busy": "2025-09-25T07:53:20.555717Z",
     "iopub.status.idle": "2025-09-25T07:53:20.694625Z",
     "shell.execute_reply": "2025-09-25T07:53:20.693853Z",
     "shell.execute_reply.started": "2025-09-25T07:53:20.555982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample src tokens: [2, 69, 1874, 27, 7030]...[42, 3522, 28, 854, 3]\n",
      "Sample tgt tokens: [2, 94, 3075, 27, 4042]...[91, 4149, 43, 961, 3]\n",
      "BOS ID: 2, EOS ID: 3\n",
      "Sample src tokens: [2, 6357, 258, 27, 407]...[27, 407, 491, 51, 3]\n",
      "Sample tgt tokens: [2, 452, 1700, 7974, 7964]...[88, 7988, 21, 79, 3]\n",
      "BOS ID: 2, EOS ID: 3\n",
      "Sample src tokens: [2, 6294, 35, 12, 435]...[378, 266, 47, 6919, 3]\n",
      "Sample tgt tokens: [2, 5740, 7964, 45, 1810]...[446, 309, 59, 5747, 3]\n",
      "BOS ID: 2, EOS ID: 3\n",
      "✅ FIXED DataLoaders ready!\n",
      "Train batches: 326\n",
      "Val batches: 164\n",
      "Test batches: 164\n",
      "\n",
      "Batch shapes: src=torch.Size([32, 14]), tgt=torch.Size([32, 16])\n",
      "Sample src: tensor([   2, 7778,  979,   85,   20,   76,  549,   32,   56,  616,  121,    3,\n",
      "           1,    1])\n",
      "Sample tgt: tensor([   2, 5783,  103, 2840,  109,  112,   99,  639,   53,   82, 1641, 7974,\n",
      "         476,    3,    1,    1])\n"
     ]
    }
   ],
   "source": [
    "# ✅ FIXED Dataset & DataLoader in PyTorch (using pre-tokenized files)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Load trained SentencePiece models\n",
    "sp_ur = spm.SentencePieceProcessor(model_file=\"/kaggle/working/spm_models/spm_ur.model\")\n",
    "sp_ro = spm.SentencePieceProcessor(model_file=\"/kaggle/working/spm_models/spm_ro.model\")\n",
    "\n",
    "# Utility function: read tokenized IDs from file\n",
    "def read_file(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [[int(tok) for tok in line.strip().split()] for line in f if line.strip()]\n",
    "\n",
    "# FIXED Custom Dataset (tokens already include BOS/EOS)\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_file, target_file):\n",
    "        self.src_ids = read_file(source_file)\n",
    "        self.tgt_ids = read_file(target_file)\n",
    "        \n",
    "        # Verify lengths match\n",
    "        assert len(self.src_ids) == len(self.tgt_ids), f\"Mismatched lengths: {len(self.src_ids)} vs {len(self.tgt_ids)}\"\n",
    "        \n",
    "        # Verify BOS/EOS tokens are already present (optional check)\n",
    "        print(f\"Sample src tokens: {self.src_ids[0][:5]}...{self.src_ids[0][-5:]}\")\n",
    "        print(f\"Sample tgt tokens: {self.tgt_ids[0][:5]}...{self.tgt_ids[0][-5:]}\")\n",
    "        print(f\"BOS ID: {sp_ur.bos_id()}, EOS ID: {sp_ur.eos_id()}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # ✅ NO DOUBLE BOS/EOS - tokens already have them!\n",
    "        src = torch.tensor(self.src_ids[idx], dtype=torch.long)\n",
    "        tgt = torch.tensor(self.tgt_ids[idx], dtype=torch.long)\n",
    "        return src, tgt\n",
    "\n",
    "# Collate function to pad batches\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    \n",
    "    # Pad sequences\n",
    "    src_padded = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=sp_ur.pad_id())\n",
    "    tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=sp_ro.pad_id())\n",
    "    \n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TranslationDataset(\"/kaggle/working/tokenized/train.src\", \"/kaggle/working/tokenized/train.tgt\")\n",
    "val_dataset   = TranslationDataset(\"/kaggle/working/tokenized/val.src\", \"/kaggle/working/tokenized/val.tgt\")\n",
    "test_dataset  = TranslationDataset(\"/kaggle/working/tokenized/test.src\", \"/kaggle/working/tokenized/test.tgt\")\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 32  # Reduced batch size for better gradient updates\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"✅ FIXED DataLoaders ready!\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Quick verification\n",
    "sample_batch = next(iter(train_loader))\n",
    "src_batch, tgt_batch = sample_batch\n",
    "print(f\"\\nBatch shapes: src={src_batch.shape}, tgt={tgt_batch.shape}\")\n",
    "print(f\"Sample src: {src_batch[0]}\")\n",
    "print(f\"Sample tgt: {tgt_batch[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Model Architecture (BiLSTM Encoder → LSTM Decoder)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Encoder (BiLSTM)\n",
    "\n",
    "**What?**  \n",
    "Converts the Urdu input sequence into hidden states.  \n",
    "\n",
    "**How?**  \n",
    "- Uses embeddings to represent tokens.  \n",
    "- BiLSTM captures both forward and backward context.  \n",
    "- Final hidden states are combined (concatenated forward + backward → passed through a fully connected layer).  \n",
    "\n",
    "**Why?**  \n",
    "Poetic Urdu often has dependencies across long spans. A BiLSTM captures richer context than a single-direction LSTM.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Decoder (LSTM)\n",
    "\n",
    "**What?**  \n",
    "Generates Roman Urdu output tokens one by one.  \n",
    "\n",
    "**How?**  \n",
    "- Embedding → LSTM → Linear layer → Vocabulary distribution.  \n",
    "- Uses teacher forcing (sometimes feeding the true token, sometimes the model’s prediction).  \n",
    "\n",
    "**Why?**  \n",
    "Teacher forcing speeds up training and stabilizes learning.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Seq2Seq Model\n",
    "\n",
    "**What?**  \n",
    "Combines Encoder and Decoder into an end-to-end NMT model.  \n",
    "\n",
    "**How?**  \n",
    "- Encoder processes the entire source sentence.  \n",
    "- Decoder starts with the `<sos>` token and generates output step by step.  \n",
    "- Teacher forcing ratio controls learning stability.  \n",
    "\n",
    "**Why?**  \n",
    "This architecture is the core requirement of the project: **BiLSTM encoder → LSTM decoder**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T07:53:31.298561Z",
     "iopub.status.busy": "2025-09-25T07:53:31.298286Z",
     "iopub.status.idle": "2025-09-25T07:53:31.320114Z",
     "shell.execute_reply": "2025-09-25T07:53:31.319394Z",
     "shell.execute_reply.started": "2025-09-25T07:53:31.298540Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Improved Encoder (BiLSTM)\n",
    "# -------------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hidden_dim, dec_hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=1)  # pad_idx=1\n",
    "        self.rnn = nn.LSTM(emb_dim, enc_hidden_dim, num_layers=n_layers,\n",
    "                          bidirectional=True, dropout=dropout if n_layers > 1 else 0.0, \n",
    "                          batch_first=True)\n",
    "        \n",
    "        # Project bidirectional hidden to decoder hidden size\n",
    "        self.fc_hidden = nn.Linear(enc_hidden_dim * 2, dec_hidden_dim)\n",
    "        self.fc_cell = nn.Linear(enc_hidden_dim * 2, dec_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len=None):\n",
    "        # src: [batch_size, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))  # [batch, src_len, emb_dim]\n",
    "        \n",
    "        # Pack if lengths provided (optional optimization)\n",
    "        if src_len is not None:\n",
    "            embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        # Unpack if packed\n",
    "        if src_len is not None:\n",
    "            outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        # outputs: [batch, src_len, enc_hidden_dim * 2]\n",
    "        # hidden: [n_layers * 2, batch, enc_hidden_dim]\n",
    "        # cell: [n_layers * 2, batch, enc_hidden_dim]\n",
    "        \n",
    "        # Concatenate final forward and backward hidden states\n",
    "        # Take the last layer's forward and backward states\n",
    "        hidden_fwd = hidden[-2, :, :]  # [batch, enc_hidden_dim]\n",
    "        hidden_bwd = hidden[-1, :, :]  # [batch, enc_hidden_dim] \n",
    "        hidden_cat = torch.cat((hidden_fwd, hidden_bwd), dim=1)  # [batch, enc_hidden_dim * 2]\n",
    "        \n",
    "        cell_fwd = cell[-2, :, :]\n",
    "        cell_bwd = cell[-1, :, :]\n",
    "        cell_cat = torch.cat((cell_fwd, cell_bwd), dim=1)\n",
    "        \n",
    "        # Project to decoder dimensions and repeat for all decoder layers\n",
    "        hidden_proj = torch.tanh(self.fc_hidden(hidden_cat))  # [batch, dec_hidden_dim]\n",
    "        cell_proj = torch.tanh(self.fc_cell(cell_cat))  # [batch, dec_hidden_dim]\n",
    "        \n",
    "        # Repeat for all decoder layers\n",
    "        hidden_proj = hidden_proj.unsqueeze(0).repeat(self.n_layers, 1, 1)  # [n_layers, batch, dec_hidden_dim]\n",
    "        cell_proj = cell_proj.unsqueeze(0).repeat(self.n_layers, 1, 1)  # [n_layers, batch, dec_hidden_dim]\n",
    "        \n",
    "        return outputs, hidden_proj, cell_proj\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Attention Mechanism\n",
    "# -------------------------------\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(enc_hidden_dim * 2 + dec_hidden_dim, dec_hidden_dim)\n",
    "        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        # hidden: [batch, dec_hidden_dim]\n",
    "        # encoder_outputs: [batch, src_len, enc_hidden_dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # Repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch, src_len, dec_hidden_dim]\n",
    "        \n",
    "        # Calculate attention energies\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch, src_len, dec_hidden_dim]\n",
    "        attention = self.v(energy).squeeze(2)  # [batch, src_len]\n",
    "        \n",
    "        # Apply mask if provided (for padding)\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Improved Decoder with Attention\n",
    "# -------------------------------\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hidden_dim, dec_hidden_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=1)\n",
    "        self.rnn = nn.LSTM(emb_dim + enc_hidden_dim * 2, dec_hidden_dim, num_layers=n_layers,\n",
    "                          dropout=dropout if n_layers > 1 else 0.0, batch_first=True)\n",
    "        \n",
    "        self.fc_out = nn.Linear(emb_dim + enc_hidden_dim * 2 + dec_hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell, encoder_outputs, mask=None):\n",
    "        # input: [batch]\n",
    "        # hidden: [n_layers, batch, dec_hidden_dim]\n",
    "        # cell: [n_layers, batch, dec_hidden_dim]\n",
    "        # encoder_outputs: [batch, src_len, enc_hidden_dim * 2]\n",
    "        \n",
    "        input = input.unsqueeze(1)  # [batch, 1]\n",
    "        embedded = self.dropout(self.embedding(input))  # [batch, 1, emb_dim]\n",
    "        \n",
    "        # Calculate attention using the top layer hidden state\n",
    "        a = self.attention(hidden[-1], encoder_outputs, mask)  # [batch, src_len]\n",
    "        a = a.unsqueeze(1)  # [batch, 1, src_len]\n",
    "        \n",
    "        # Calculate weighted encoder outputs (context vector)\n",
    "        weighted = torch.bmm(a, encoder_outputs)  # [batch, 1, enc_hidden_dim * 2]\n",
    "        \n",
    "        # Concatenate embedding with context\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)  # [batch, 1, emb_dim + enc_hidden_dim * 2]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        # output: [batch, 1, dec_hidden_dim]\n",
    "        \n",
    "        # Prediction\n",
    "        prediction = self.fc_out(torch.cat((output.squeeze(1), weighted.squeeze(1), embedded.squeeze(1)), dim=1))\n",
    "        # [batch, output_dim]\n",
    "        \n",
    "        return prediction, hidden, cell, a.squeeze(1)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Complete Seq2Seq Model\n",
    "# -------------------------------\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        # Create mask for padding tokens\n",
    "        mask = (src != self.src_pad_idx)  # [batch, src_len]\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # Create mask for source padding\n",
    "        mask = self.create_mask(src)\n",
    "        \n",
    "        # Encode\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # Initialize outputs tensor\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size, device=self.device)\n",
    "        attentions = torch.zeros(batch_size, trg_len, src.shape[1], device=self.device)\n",
    "        \n",
    "        # First input to decoder is the BOS token\n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            # Pass through decoder\n",
    "            output, hidden, cell, attention = self.decoder(input, hidden, cell, encoder_outputs, mask)\n",
    "            \n",
    "            # Store output and attention\n",
    "            outputs[:, t] = output\n",
    "            attentions[:, t] = attention\n",
    "            \n",
    "            # Decide whether to use teacher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # Get the highest predicted token\n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            # Update input for next time step\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "            \n",
    "        return outputs, attentions\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Model Initialization Function\n",
    "# -------------------------------\n",
    "def create_model(src_vocab_size, trg_vocab_size, device, \n",
    "                emb_dim=256, enc_hidden_dim=256, dec_hidden_dim=256, \n",
    "                n_layers=2, dropout=0.3, src_pad_idx=1):\n",
    "    \n",
    "    attention = Attention(enc_hidden_dim, dec_hidden_dim)\n",
    "    encoder = Encoder(src_vocab_size, emb_dim, enc_hidden_dim, dec_hidden_dim, n_layers, dropout)\n",
    "    decoder = Decoder(trg_vocab_size, emb_dim, enc_hidden_dim, dec_hidden_dim, n_layers, dropout, attention)\n",
    "    \n",
    "    model = Seq2Seq(encoder, decoder, src_pad_idx, device).to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Model Parameter Initialization\n",
    "# -------------------------------\n",
    "def init_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "# Usage example:\n",
    "# model = create_model(src_vocab_size=8000, trg_vocab_size=8000, device=device)\n",
    "# init_weights(model)\n",
    "# print(f\"Model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup (Loss, Optimizer, Hyperparameters)\n",
    "\n",
    "---\n",
    "\n",
    "### What we are doing\n",
    "We set up the model with vocabulary sizes, dimensions, loss function, optimizer, and training parameters.  \n",
    "\n",
    "---\n",
    "\n",
    "### How\n",
    "- SentencePiece provides `ur_vocab_size` and `ro_vocab_size`.  \n",
    "- Encoder and Decoder are initialized with BiLSTM layers and dropout.  \n",
    "- **Loss function**: `CrossEntropyLoss`, ignoring `<pad>` so padding does not affect training.  \n",
    "- **Optimizer**: Adam, which adapts the learning rate automatically.  \n",
    "- **Training hyperparameters**:  \n",
    "  - Batch size  \n",
    "  - Number of epochs  \n",
    "  - Teacher forcing ratio (controls how often the decoder uses the true token vs. its own prediction)  \n",
    "\n",
    "---\n",
    "\n",
    "### Why\n",
    "- Padding should not influence the gradient updates, so we mask it in the loss.  \n",
    "- Adam optimizer generally works well for sequence models due to its adaptive learning rate.  \n",
    "- Proper hyperparameter settings (batch size, epochs, teacher forcing ratio) are crucial for stable training and good performance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:58:21.017560Z",
     "iopub.status.busy": "2025-09-25T08:58:21.016781Z",
     "iopub.status.idle": "2025-09-25T08:58:21.241523Z",
     "shell.execute_reply": "2025-09-25T08:58:21.240900Z",
     "shell.execute_reply.started": "2025-09-25T08:58:21.017524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urdu vocab size: 8000\n",
      "Roman vocab size: 8000\n",
      "Using device: cuda\n",
      "The model has 25,267,520 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE FIXED TRAINING SETUP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math\n",
    "import time\n",
    "# --------------------------\n",
    "# 1. Load vocab sizes from SentencePiece\n",
    "# --------------------------\n",
    "ur_vocab_size = sp_ur.get_piece_size()   # Urdu vocab size\n",
    "ro_vocab_size = sp_ro.get_piece_size()   # Roman Urdu vocab size\n",
    "print(\"Urdu vocab size:\", ur_vocab_size)\n",
    "print(\"Roman vocab size:\", ro_vocab_size)\n",
    "# --------------------------\n",
    "# 2. FIXED Hyperparameters\n",
    "# --------------------------\n",
    "# Model Architecture\n",
    "EMB_DIM = 256           # ✅ Increased from 128\n",
    "ENC_HIDDEN_DIM = 384    # ✅ Encoder hidden dimension\n",
    "DEC_HIDDEN_DIM = 384   # ✅ Decoder hidden dimension\n",
    "N_LAYERS = 2            # ✅ Same for encoder and decoder\n",
    "DROPOUT = 0.3           # ✅ Reduced from 0.4\n",
    "# Training Parameters\n",
    "LEARNING_RATE = 0.001   # ✅ Increased from 1e-4\n",
    "BATCH_SIZE = 32         # ✅ Match your dataloader\n",
    "NUM_EPOCHS = 30         # ✅ More epochs\n",
    "TEACHER_FORCING_RATIO = 0.6  # ✅ Start lower\n",
    "CLIP = 0.5              # ✅ Gradient clipping\n",
    "PATIENCE = 5            # ✅ Early stopping patience\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# --------------------------\n",
    "# 3. Initialize IMPROVED Model\n",
    "# --------------------------\n",
    "# Use the improved model architecture I provided earlier\n",
    "model = create_model(\n",
    "    src_vocab_size=ur_vocab_size,\n",
    "    trg_vocab_size=ro_vocab_size,\n",
    "    device=device,\n",
    "    emb_dim=EMB_DIM,\n",
    "    enc_hidden_dim=ENC_HIDDEN_DIM,\n",
    "    dec_hidden_dim=DEC_HIDDEN_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    src_pad_idx=sp_ur.pad_id()  # ✅ Proper pad index\n",
    ")\n",
    "# Initialize weights\n",
    "init_weights(model)\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "# --------------------------\n",
    "# 4. Define Loss Function & Optimizer\n",
    "# --------------------------\n",
    "PAD_IDX = sp_ro.pad_id()  # Roman Urdu pad token\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "# ✅ Better optimizer with weight decay\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                       lr=LEARNING_RATE, \n",
    "                       weight_decay=1e-5,\n",
    "                       betas=(0.9, 0.98),\n",
    "                       eps=1e-9)\n",
    "# ✅ Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:58:29.536989Z",
     "iopub.status.busy": "2025-09-25T08:58:29.536717Z",
     "iopub.status.idle": "2025-09-25T09:05:13.726002Z",
     "shell.execute_reply": "2025-09-25T09:05:13.725386Z",
     "shell.execute_reply.started": "2025-09-25T08:58:29.536969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "--------------------------------------------------\n",
      "Epoch 01/30\n",
      "  Batch 0/326, Loss: 8.9873\n",
      "  Batch 50/326, Loss: 6.3916\n",
      "  Batch 100/326, Loss: 6.1001\n",
      "  Batch 150/326, Loss: 6.0167\n",
      "  Batch 200/326, Loss: 5.6326\n",
      "  Batch 250/326, Loss: 5.9795\n",
      "  Batch 300/326, Loss: 5.5891\n",
      "  ✅ New best model saved!\n",
      "  Time: 0m 29s\n",
      "  Train Loss: 5.9757 | Valid Loss: 6.1227\n",
      "  Learning Rate: 0.001000\n",
      "  Best Valid Loss: 6.1227\n",
      "  Patience: 0/5\n",
      "--------------------------------------------------\n",
      "Epoch 02/30\n",
      "  Batch 0/326, Loss: 5.1740\n",
      "  Batch 50/326, Loss: 5.3226\n",
      "  Batch 100/326, Loss: 4.9432\n",
      "  Batch 150/326, Loss: 4.9394\n",
      "  Batch 200/326, Loss: 5.0912\n",
      "  Batch 250/326, Loss: 4.7332\n",
      "  Batch 300/326, Loss: 4.2444\n",
      "  ✅ New best model saved!\n",
      "  Time: 0m 30s\n",
      "  Train Loss: 4.9673 | Valid Loss: 5.1839\n",
      "  Learning Rate: 0.001000\n",
      "  Best Valid Loss: 5.1839\n",
      "  Patience: 0/5\n",
      "--------------------------------------------------\n",
      "Epoch 03/30\n",
      "  Batch 0/326, Loss: 4.1789\n",
      "  Batch 50/326, Loss: 3.9374\n",
      "  Batch 100/326, Loss: 3.7983\n",
      "  Batch 150/326, Loss: 3.7179\n",
      "  Batch 200/326, Loss: 3.7783\n",
      "  Batch 250/326, Loss: 3.3749\n",
      "  Batch 300/326, Loss: 3.4338\n",
      "  ✅ New best model saved!\n",
      "  Time: 0m 30s\n",
      "  Train Loss: 3.7883 | Valid Loss: 4.5479\n",
      "  Learning Rate: 0.001000\n",
      "  Best Valid Loss: 4.5479\n",
      "  Patience: 0/5\n",
      "--------------------------------------------------\n",
      "Epoch 04/30\n",
      "  Batch 0/326, Loss: 2.8729\n",
      "  Batch 50/326, Loss: 3.1705\n",
      "  Batch 100/326, Loss: 3.2668\n",
      "  Batch 150/326, Loss: 3.1796\n",
      "  Batch 200/326, Loss: 2.8142\n",
      "  Batch 250/326, Loss: 2.8406\n",
      "  Batch 300/326, Loss: 3.0065\n",
      "  ✅ New best model saved!\n",
      "  Time: 0m 30s\n",
      "  Train Loss: 2.9373 | Valid Loss: 4.2901\n",
      "  Learning Rate: 0.001000\n",
      "  Best Valid Loss: 4.2901\n",
      "  Patience: 0/5\n",
      "--------------------------------------------------\n",
      "Epoch 05/30\n",
      "  Batch 0/326, Loss: 2.9127\n",
      "  Batch 50/326, Loss: 2.4130\n",
      "  Batch 100/326, Loss: 2.0540\n",
      "  Batch 150/326, Loss: 2.4100\n",
      "  Batch 200/326, Loss: 2.0199\n",
      "  Batch 250/326, Loss: 2.4254\n",
      "  Batch 300/326, Loss: 2.6107\n",
      "  ✅ New best model saved!\n",
      "  Time: 0m 30s\n",
      "  Train Loss: 2.3112 | Valid Loss: 3.9739\n",
      "  Learning Rate: 0.001000\n",
      "  Best Valid Loss: 3.9739\n",
      "  Patience: 0/5\n",
      "--------------------------------------------------\n",
      "Epoch 06/30\n",
      "  Batch 0/326, Loss: 1.6367\n",
      "  Batch 50/326, Loss: 1.7541\n",
      "  Batch 100/326, Loss: 1.9078\n",
      "  Batch 150/326, Loss: 1.9661\n",
      "  Batch 200/326, Loss: 1.8435\n",
      "  Batch 250/326, Loss: 1.9316\n",
      "  Batch 300/326, Loss: 2.1626\n",
      "  ✅ New best model saved!\n",
      "  Time: 0m 30s\n",
      "  Train Loss: 1.8345 | Valid Loss: 3.7351\n",
      "  Learning Rate: 0.001000\n",
      "  Best Valid Loss: 3.7351\n",
      "  Patience: 0/5\n",
      "--------------------------------------------------\n",
      "Epoch 07/30\n",
      "  Batch 0/326, Loss: 1.6555\n",
      "  Batch 50/326, Loss: 1.1196\n",
      "  Batch 100/326, Loss: 1.4452\n",
      "  Batch 150/326, Loss: 1.2835\n",
      "  Batch 200/326, Loss: 1.7525\n",
      "  Batch 250/326, Loss: 1.5048\n",
      "  Batch 300/326, Loss: 1.7686\n",
      "  ✅ New best model saved!\n",
      "  Time: 0m 30s\n",
      "  Train Loss: 1.4354 | Valid Loss: 3.7116\n",
      "  Learning Rate: 0.001000\n",
      "  Best Valid Loss: 3.7116\n",
      "  Patience: 0/5\n",
      "--------------------------------------------------\n",
      "Epoch 08/30\n",
      "  Batch 0/326, Loss: 0.9531\n",
      "  Batch 50/326, Loss: 1.2369\n",
      "  Batch 100/326, Loss: 1.1252\n",
      "  Batch 150/326, Loss: 0.9050\n",
      "  Batch 200/326, Loss: 0.9232\n",
      "  Batch 250/326, Loss: 1.4764\n",
      "  Batch 300/326, Loss: 1.3185\n",
      "  ✅ New best model saved!\n",
      "  Time: 0m 30s\n",
      "  Train Loss: 1.1184 | Valid Loss: 3.6866\n",
      "  Learning Rate: 0.001000\n",
      "  Best Valid Loss: 3.6866\n",
      "  Patience: 0/5\n",
      "--------------------------------------------------\n",
      "Epoch 09/30\n",
      "  Batch 0/326, Loss: 0.8444\n",
      "  Batch 50/326, Loss: 0.6303\n",
      "  Batch 100/326, Loss: 0.8616\n",
      "  Batch 150/326, Loss: 1.0581\n",
      "  Batch 200/326, Loss: 0.8067\n",
      "  Batch 250/326, Loss: 0.9430\n",
      "  Batch 300/326, Loss: 1.1952\n",
      "  Time: 0m 30s\n",
      "  Train Loss: 0.8471 | Valid Loss: 3.6958\n",
      "  Learning Rate: 0.001000\n",
      "  Best Valid Loss: 3.6866\n",
      "  Patience: 1/5\n",
      "--------------------------------------------------\n",
      "Epoch 10/30\n",
      "  Batch 0/326, Loss: 0.6214\n",
      "  Batch 50/326, Loss: 0.6273\n",
      "  Batch 100/326, Loss: 0.6220\n",
      "  Batch 150/326, Loss: 0.6623\n",
      "  Batch 200/326, Loss: 0.5554\n",
      "  Batch 250/326, Loss: 0.7425\n",
      "  Batch 300/326, Loss: 0.5329\n",
      "  Time: 0m 30s\n",
      "  Train Loss: 0.6353 | Valid Loss: 3.8083\n",
      "  Learning Rate: 0.001000\n",
      "  Best Valid Loss: 3.6866\n",
      "  Patience: 2/5\n",
      "--------------------------------------------------\n",
      "Epoch 11/30\n",
      "  Batch 0/326, Loss: 0.7365\n",
      "  Batch 50/326, Loss: 0.5282\n",
      "  Batch 100/326, Loss: 0.3590\n",
      "  Batch 150/326, Loss: 0.4502\n",
      "  Batch 200/326, Loss: 0.6752\n",
      "  Batch 250/326, Loss: 0.5071\n",
      "  Batch 300/326, Loss: 0.7037\n",
      "  Time: 0m 30s\n",
      "  Train Loss: 0.4997 | Valid Loss: 3.9091\n",
      "  Learning Rate: 0.001000\n",
      "  Best Valid Loss: 3.6866\n",
      "  Patience: 3/5\n",
      "--------------------------------------------------\n",
      "Epoch 12/30\n",
      "  Batch 0/326, Loss: 0.2397\n",
      "  Batch 50/326, Loss: 0.3044\n",
      "  Batch 100/326, Loss: 0.3257\n",
      "  Batch 150/326, Loss: 0.2712\n",
      "  Batch 200/326, Loss: 0.4006\n",
      "  Batch 250/326, Loss: 0.2285\n",
      "  Batch 300/326, Loss: 0.6609\n",
      "  Time: 0m 30s\n",
      "  Train Loss: 0.4042 | Valid Loss: 3.9074\n",
      "  Learning Rate: 0.001000\n",
      "  Best Valid Loss: 3.6866\n",
      "  Patience: 4/5\n",
      "--------------------------------------------------\n",
      "Epoch 13/30\n",
      "  Batch 0/326, Loss: 0.1955\n",
      "  Batch 50/326, Loss: 0.2482\n",
      "  Batch 100/326, Loss: 0.2960\n",
      "  Batch 150/326, Loss: 0.5049\n",
      "  Batch 200/326, Loss: 0.3865\n",
      "  Batch 250/326, Loss: 0.4139\n",
      "  Batch 300/326, Loss: 0.3835\n",
      "  Time: 0m 30s\n",
      "  Train Loss: 0.3176 | Valid Loss: 4.0652\n",
      "  Learning Rate: 0.001000\n",
      "  Best Valid Loss: 3.6866\n",
      "  Patience: 5/5\n",
      "--------------------------------------------------\n",
      "Early stopping triggered after 13 epochs\n",
      "Training complete!\n",
      "Best validation loss: 3.6866\n",
      "Loading best model for evaluation...\n",
      "Test Loss: 3.6948\n",
      "Test Perplexity: 40.2395\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 5. IMPROVED Training Functions\n",
    "# --------------------------\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, clip, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for batch_idx, (src, tgt) in enumerate(dataloader):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass - returns (outputs, attentions)\n",
    "        outputs, _ = model(src, tgt, teacher_forcing_ratio=TEACHER_FORCING_RATIO)\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        # outputs: [batch_size, trg_len, vocab_size]\n",
    "        output_dim = outputs.shape[-1]\n",
    "        \n",
    "        # Skip BOS token (index 0) for loss calculation\n",
    "        outputs = outputs[:, 1:].contiguous().view(-1, output_dim)\n",
    "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, tgt)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss and token count\n",
    "        epoch_loss += loss.item()\n",
    "        total_tokens += (tgt != PAD_IDX).sum().item()\n",
    "        \n",
    "        # Print progress every 50 batches\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'  Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # No teacher forcing during evaluation\n",
    "            outputs, _ = model(src, tgt, teacher_forcing_ratio=0.0)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            output_dim = outputs.shape[-1]\n",
    "            outputs = outputs[:, 1:].contiguous().view(-1, output_dim)\n",
    "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(outputs, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "            total_tokens += (tgt != PAD_IDX).sum().item()\n",
    "    \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "# --------------------------\n",
    "# 6. TRAINING LOOP with Early Stopping\n",
    "# --------------------------\n",
    "SAVE_PATH = \"/kaggle/working/best_seq2seq_model.pt\"\n",
    "best_valid_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f'Epoch {epoch+1:02}/{NUM_EPOCHS}')\n",
    "    \n",
    "    # Training\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, CLIP, device)\n",
    "    \n",
    "    # Validation\n",
    "    valid_loss = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # Save best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_valid_loss': best_valid_loss,\n",
    "            'hyperparameters': {\n",
    "                'emb_dim': EMB_DIM,\n",
    "                'enc_hidden_dim': ENC_HIDDEN_DIM,\n",
    "                'dec_hidden_dim': DEC_HIDDEN_DIM,\n",
    "                'n_layers': N_LAYERS,\n",
    "                'dropout': DROPOUT\n",
    "            }\n",
    "        }, SAVE_PATH)\n",
    "        print(f'  ✅ New best model saved!')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f'  Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'  Train Loss: {train_loss:.4f} | Valid Loss: {valid_loss:.4f}')\n",
    "    print(f'  Learning Rate: {current_lr:.6f}')\n",
    "    print(f'  Best Valid Loss: {best_valid_loss:.4f}')\n",
    "    print(f'  Patience: {patience_counter}/{PATIENCE}')\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(f\"Best validation loss: {best_valid_loss:.4f}\")\n",
    "\n",
    "# --------------------------\n",
    "# 7. Load Best Model for Testing\n",
    "# --------------------------\n",
    "print(\"Loading best model for evaluation...\")\n",
    "checkpoint = torch.load(SAVE_PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Test evaluation\n",
    "test_loss = evaluate(model, test_loader, criterion, device)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T09:05:51.708530Z",
     "iopub.status.busy": "2025-09-25T09:05:51.708239Z",
     "iopub.status.idle": "2025-09-25T09:08:27.244498Z",
     "shell.execute_reply": "2025-09-25T09:08:27.243712Z",
     "shell.execute_reply.started": "2025-09-25T09:05:51.708509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocabularies: Urdu=8000, Roman=8000\n",
      "✅ Loaded model from checkpoint\n",
      "Starting evaluation...\n",
      "Evaluating 5219 sentences...\n",
      "Processed 0 sentences...\n",
      "\n",
      "Example 1:\n",
      "Source:     استان یار سے اٹھ جائیں کیا\n",
      "Reference:  āstān-e-yār se uth jaa.eñ kyā\n",
      "Greedy:     pūchhiye yaar se uth jaa.e kyā kyā\n",
      "Beam(5):    ḳhūb-e se yaar jaa.e kyā kyā\n",
      "Improvement: 🔄\n",
      "\n",
      "Example 2:\n",
      "Source:     اتا ہے ہوش مجھ کو اب تو پہر پہر میں\n",
      "Reference:  aatā hai hosh mujh ko ab to pahr pahr meñ\n",
      "Greedy:     aatā hai jin mujh ko ab to dikhāyā pesh meñ\n",
      "Beam(5):    aatā hai jin mujh ko ab to dikhāyāgī meñ\n",
      "Improvement: 🔄\n",
      "\n",
      "Example 3:\n",
      "Source:     صبح کے درد کو راتوں کی جلن کو بھولیں\n",
      "Reference:  sub.h ke dard ko rātoñ kī jalan ko bhūleñ\n",
      "Greedy:     sub. dard ke dard ko pahchān kī jal. ko ko ko\n",
      "Beam(5):    sub. dard ke dard ko pahchān kī jal.an ko bhūleñ\n",
      "Improvement: 🔄\n",
      "\n",
      "Example 4:\n",
      "Source:     یہ ہمیں تھے جن کے لباس پر سر رہ سیاہی لکھی گئی\n",
      "Reference:  ye hamīñ the jin ke libās par sar-e-rah siyāhī likhī ga.ī\n",
      "Greedy:     ye hameñ the jin ke ḳhas par sar-e-rah-- ga.ī ga.ī\n",
      "Beam(5):    ye hameñ the jin ke ḳhas par sar-e-rah-- ga.ī ga.ī ga.ī\n",
      "Improvement: ✅\n",
      "\n",
      "Example 5:\n",
      "Source:     خدا کی خدائی میں تجھ سا نہ دیکھا\n",
      "Reference:  ḳhudā kī ḳhudā.ī meñ tujh sā na dekhā\n",
      "Greedy:     ḳhudā kī ḳhudā meñ tujh sā na dekhā\n",
      "Beam(5):    ḳhudā kī ḳhudā meñ tujh sā na dekhā\n",
      "Improvement: 🔄\n",
      "Processed 100 sentences...\n",
      "Processed 200 sentences...\n",
      "Processed 300 sentences...\n",
      "Processed 400 sentences...\n",
      "Processed 500 sentences...\n",
      "Processed 600 sentences...\n",
      "Processed 700 sentences...\n",
      "Processed 800 sentences...\n",
      "Processed 900 sentences...\n",
      "Processed 1000 sentences...\n",
      "Stopping evaluation at 1001 sentences for efficiency...\n",
      "\n",
      "Calculating metrics for 1001 sentences...\n",
      "BLEU Score: 38.14\n",
      "CER: 0.3486 (34.86%)\n",
      "WER: 0.3816 (38.16%)\n",
      "\n",
      "✅ Evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import math\n",
    "import sentencepiece as smp\n",
    "import editdistance   # pip install editdistance\n",
    "import random\n",
    "\n",
    "# =====================================================\n",
    "# 1. Load SentencePiece Models\n",
    "# =====================================================\n",
    "sp_ur = spm.SentencePieceProcessor(model_file=\"spm_models/spm_ur.model\")\n",
    "sp_ro = spm.SentencePieceProcessor(model_file=\"spm_models/spm_ro.model\")\n",
    "\n",
    "ur_vocab_size = sp_ur.get_piece_size()\n",
    "ro_vocab_size = sp_ro.get_piece_size()\n",
    "\n",
    "print(f\"Loaded vocabularies: Urdu={ur_vocab_size}, Roman={ro_vocab_size}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. UPDATED Model Architecture (same as training)\n",
    "# =====================================================\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hidden_dim, dec_hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=1)  # pad_idx=1\n",
    "        self.rnn = nn.LSTM(emb_dim, enc_hidden_dim, num_layers=n_layers,\n",
    "                          bidirectional=True, dropout=dropout if n_layers > 1 else 0.0, \n",
    "                          batch_first=True)\n",
    "        \n",
    "        # Project bidirectional hidden to decoder hidden size\n",
    "        self.fc_hidden = nn.Linear(enc_hidden_dim * 2, dec_hidden_dim)\n",
    "        self.fc_cell = nn.Linear(enc_hidden_dim * 2, dec_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len=None):\n",
    "        # src: [batch_size, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))  # [batch, src_len, emb_dim]\n",
    "        \n",
    "        # Pack if lengths provided (optional optimization)\n",
    "        if src_len is not None:\n",
    "            embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        # Unpack if packed\n",
    "        if src_len is not None:\n",
    "            outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        # outputs: [batch, src_len, enc_hidden_dim * 2]\n",
    "        # hidden: [n_layers * 2, batch, enc_hidden_dim]\n",
    "        # cell: [n_layers * 2, batch, enc_hidden_dim]\n",
    "        \n",
    "        # Concatenate final forward and backward hidden states\n",
    "        # Take the last layer's forward and backward states\n",
    "        hidden_fwd = hidden[-2, :, :]  # [batch, enc_hidden_dim]\n",
    "        hidden_bwd = hidden[-1, :, :]  # [batch, enc_hidden_dim] \n",
    "        hidden_cat = torch.cat((hidden_fwd, hidden_bwd), dim=1)  # [batch, enc_hidden_dim * 2]\n",
    "        \n",
    "        cell_fwd = cell[-2, :, :]\n",
    "        cell_bwd = cell[-1, :, :]\n",
    "        cell_cat = torch.cat((cell_fwd, cell_bwd), dim=1)\n",
    "        \n",
    "        # Project to decoder dimensions and repeat for all decoder layers\n",
    "        hidden_proj = torch.tanh(self.fc_hidden(hidden_cat))  # [batch, dec_hidden_dim]\n",
    "        cell_proj = torch.tanh(self.fc_cell(cell_cat))  # [batch, dec_hidden_dim]\n",
    "        \n",
    "        # Repeat for all decoder layers\n",
    "        hidden_proj = hidden_proj.unsqueeze(0).repeat(self.n_layers, 1, 1)  # [n_layers, batch, dec_hidden_dim]\n",
    "        cell_proj = cell_proj.unsqueeze(0).repeat(self.n_layers, 1, 1)  # [n_layers, batch, dec_hidden_dim]\n",
    "        \n",
    "        return outputs, hidden_proj, cell_proj\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(enc_hidden_dim * 2 + dec_hidden_dim, dec_hidden_dim)\n",
    "        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        # hidden: [batch, dec_hidden_dim]\n",
    "        # encoder_outputs: [batch, src_len, enc_hidden_dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # Repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch, src_len, dec_hidden_dim]\n",
    "        \n",
    "        # Calculate attention energies\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch, src_len, dec_hidden_dim]\n",
    "        attention = self.v(energy).squeeze(2)  # [batch, src_len]\n",
    "        \n",
    "        # Apply mask if provided (for padding)\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hidden_dim, dec_hidden_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=1)\n",
    "        self.rnn = nn.LSTM(emb_dim + enc_hidden_dim * 2, dec_hidden_dim, num_layers=n_layers,\n",
    "                          dropout=dropout if n_layers > 1 else 0.0, batch_first=True)\n",
    "        \n",
    "        self.fc_out = nn.Linear(emb_dim + enc_hidden_dim * 2 + dec_hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell, encoder_outputs, mask=None):\n",
    "        # input: [batch]\n",
    "        # hidden: [n_layers, batch, dec_hidden_dim]\n",
    "        # cell: [n_layers, batch, dec_hidden_dim]\n",
    "        # encoder_outputs: [batch, src_len, enc_hidden_dim * 2]\n",
    "        \n",
    "        input = input.unsqueeze(1)  # [batch, 1]\n",
    "        embedded = self.dropout(self.embedding(input))  # [batch, 1, emb_dim]\n",
    "        \n",
    "        # Calculate attention using the top layer hidden state\n",
    "        a = self.attention(hidden[-1], encoder_outputs, mask)  # [batch, src_len]\n",
    "        a = a.unsqueeze(1)  # [batch, 1, src_len]\n",
    "        \n",
    "        # Calculate weighted encoder outputs (context vector)\n",
    "        weighted = torch.bmm(a, encoder_outputs)  # [batch, 1, enc_hidden_dim * 2]\n",
    "        \n",
    "        # Concatenate embedding with context\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)  # [batch, 1, emb_dim + enc_hidden_dim * 2]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        # output: [batch, 1, dec_hidden_dim]\n",
    "        \n",
    "        # Prediction\n",
    "        prediction = self.fc_out(torch.cat((output.squeeze(1), weighted.squeeze(1), embedded.squeeze(1)), dim=1))\n",
    "        # [batch, output_dim]\n",
    "        \n",
    "        return prediction, hidden, cell, a.squeeze(1)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        # Create mask for padding tokens\n",
    "        mask = (src != self.src_pad_idx)  # [batch, src_len]\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # Create mask for source padding\n",
    "        mask = self.create_mask(src)\n",
    "        \n",
    "        # Encode\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # Initialize outputs tensor\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size, device=self.device)\n",
    "        attentions = torch.zeros(batch_size, trg_len, src.shape[1], device=self.device)\n",
    "        \n",
    "        # First input to decoder is the BOS token\n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            # Pass through decoder\n",
    "            output, hidden, cell, attention = self.decoder(input, hidden, cell, encoder_outputs, mask)\n",
    "            \n",
    "            # Store output and attention\n",
    "            outputs[:, t] = output\n",
    "            attentions[:, t] = attention\n",
    "            \n",
    "            # Decide whether to use teacher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # Get the highest predicted token\n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            # Update input for next time step\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "            \n",
    "        return outputs, attentions\n",
    "\n",
    "def create_model(src_vocab_size, trg_vocab_size, device, \n",
    "                emb_dim=256, enc_hidden_dim=256, dec_hidden_dim=256, \n",
    "                n_layers=2, dropout=0.3, src_pad_idx=1):\n",
    "    \n",
    "    attention = Attention(enc_hidden_dim, dec_hidden_dim)\n",
    "    encoder = Encoder(src_vocab_size, emb_dim, enc_hidden_dim, dec_hidden_dim, n_layers, dropout)\n",
    "    decoder = Decoder(trg_vocab_size, emb_dim, enc_hidden_dim, dec_hidden_dim, n_layers, dropout, attention)\n",
    "    \n",
    "    model = Seq2Seq(encoder, decoder, src_pad_idx, device).to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# =====================================================\n",
    "# 3. Load Trained Model with CORRECT Parameters\n",
    "# =====================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ✅ UPDATED Hyperparameters (match your training)\n",
    "EMB_DIM = 256           \n",
    "ENC_HIDDEN_DIM = 384    \n",
    "DEC_HIDDEN_DIM = 384    \n",
    "N_LAYERS = 2            \n",
    "DROPOUT = 0.3   \n",
    "\n",
    "# Create model with updated architecture\n",
    "model = create_model(\n",
    "    src_vocab_size=ur_vocab_size,\n",
    "    trg_vocab_size=ro_vocab_size,\n",
    "    device=device,\n",
    "    emb_dim=EMB_DIM,\n",
    "    enc_hidden_dim=ENC_HIDDEN_DIM,\n",
    "    dec_hidden_dim=DEC_HIDDEN_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    src_pad_idx=sp_ur.pad_id()\n",
    ")\n",
    "\n",
    "# Load the trained weights\n",
    "try:\n",
    "    checkpoint = torch.load(\"/kaggle/working/best_seq2seq_model.pt\", map_location=device)\n",
    "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"✅ Loaded model from checkpoint\")\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"✅ Loaded model state dict directly\")\n",
    "    model.eval()\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Model file not found! Please check the path.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# =====================================================\n",
    "# 4. BEAM SEARCH AND TRANSLATION FUNCTIONS\n",
    "# =====================================================\n",
    "from collections import namedtuple\n",
    "\n",
    "BeamNode = namedtuple('BeamNode', ['tokens', 'log_prob', 'hidden', 'cell', 'finished'])\n",
    "\n",
    "def beam_search_translate(model, sentence, sp_src, sp_trg, beam_size=5, max_len=50, device='cpu'):\n",
    "    \"\"\"\n",
    "    Beam search translation - significantly better than greedy search\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input sentence\n",
    "    tokens = [sp_src.bos_id()] + sp_src.encode(sentence.strip(), out_type=int) + [sp_src.eos_id()]\n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Create mask and encode\n",
    "        mask = model.create_mask(src_tensor)\n",
    "        encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
    "        \n",
    "        # Initialize beam with BOS token\n",
    "        initial_token = sp_trg.bos_id()\n",
    "        beams = [BeamNode(\n",
    "            tokens=[initial_token],\n",
    "            log_prob=0.0,\n",
    "            hidden=hidden,\n",
    "            cell=cell,\n",
    "            finished=False\n",
    "        )]\n",
    "        \n",
    "        finished_beams = []\n",
    "        \n",
    "        # Generate tokens step by step\n",
    "        for step in range(max_len):\n",
    "            if not beams:\n",
    "                break\n",
    "                \n",
    "            candidates = []\n",
    "            \n",
    "            for beam in beams:\n",
    "                if beam.finished:\n",
    "                    finished_beams.append(beam)\n",
    "                    continue\n",
    "                \n",
    "                # Get predictions for current beam\n",
    "                input_token = torch.LongTensor([beam.tokens[-1]]).to(device)\n",
    "                output, new_hidden, new_cell, _ = model.decoder(\n",
    "                    input_token, beam.hidden, beam.cell, encoder_outputs, mask\n",
    "                )\n",
    "                \n",
    "                # Get top beam_size candidates\n",
    "                log_probs = F.log_softmax(output, dim=-1)\n",
    "                top_log_probs, top_indices = torch.topk(log_probs, beam_size)\n",
    "                \n",
    "                # Expand beam\n",
    "                for i in range(beam_size):\n",
    "                    token_id = top_indices[0, i].item()\n",
    "                    token_log_prob = top_log_probs[0, i].item()\n",
    "                    \n",
    "                    new_tokens = beam.tokens + [token_id]\n",
    "                    new_log_prob = beam.log_prob + token_log_prob\n",
    "                    \n",
    "                    # Check if finished\n",
    "                    is_finished = (token_id == sp_trg.eos_id())\n",
    "                    \n",
    "                    candidates.append(BeamNode(\n",
    "                        tokens=new_tokens,\n",
    "                        log_prob=new_log_prob,\n",
    "                        hidden=new_hidden.clone(),\n",
    "                        cell=new_cell.clone(),\n",
    "                        finished=is_finished\n",
    "                    ))\n",
    "            \n",
    "            # Keep top beam_size candidates (length normalized)\n",
    "            candidates.sort(key=lambda x: x.log_prob / len(x.tokens), reverse=True)\n",
    "            beams = [beam for beam in candidates[:beam_size] if not beam.finished]\n",
    "            \n",
    "            # Add finished beams\n",
    "            finished_beams.extend([beam for beam in candidates if beam.finished])\n",
    "        \n",
    "        # Add remaining beams to finished\n",
    "        finished_beams.extend(beams)\n",
    "        \n",
    "        if not finished_beams:\n",
    "            return \"\"\n",
    "        \n",
    "        # Get best beam (length normalized score)\n",
    "        best_beam = max(finished_beams, key=lambda x: x.log_prob / len(x.tokens))\n",
    "        \n",
    "        # Decode tokens (remove BOS/EOS)\n",
    "        decoded_tokens = [t for t in best_beam.tokens[1:] if t != sp_trg.eos_id()]\n",
    "        return sp_trg.decode(decoded_tokens).strip()\n",
    "\n",
    "def translate_sentence(sentence, model, sp_src, sp_trg, max_len=50, device='cpu'):\n",
    "    \"\"\"Greedy search translation (original method)\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input sentence\n",
    "    tokens = [sp_src.bos_id()] + sp_src.encode(sentence.strip(), out_type=int) + [sp_src.eos_id()]\n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Create mask\n",
    "        mask = model.create_mask(src_tensor)\n",
    "        \n",
    "        # Encode\n",
    "        encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
    "\n",
    "    # Start with BOS token\n",
    "    trg_indexes = [sp_trg.bos_id()]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Use decoder with attention\n",
    "            output, hidden, cell, _ = model.decoder(trg_tensor, hidden, cell, encoder_outputs, mask)\n",
    "            pred_token = output.argmax(1).item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "        \n",
    "        # Stop if EOS token is generated\n",
    "        if pred_token == sp_trg.eos_id():\n",
    "            break\n",
    "    \n",
    "    # Decode without BOS/EOS tokens\n",
    "    translated_tokens = [t for t in trg_indexes[1:] if t != sp_trg.eos_id()]\n",
    "    translated = sp_trg.decode(translated_tokens)\n",
    "    return translated.strip()\n",
    "\n",
    "def translate_sentence_improved(sentence, model, sp_src, sp_trg, use_beam_search=True, beam_size=5, max_len=50, device='cpu'):\n",
    "    \"\"\"\n",
    "    Improved translation with beam search option\n",
    "    \"\"\"\n",
    "    if use_beam_search:\n",
    "        return beam_search_translate(model, sentence, sp_src, sp_trg, beam_size, max_len, device)\n",
    "    else:\n",
    "        return translate_sentence(sentence, model, sp_src, sp_trg, max_len, device)\n",
    "\n",
    "def calculate_bleu_correct(references, hypotheses):\n",
    "    \"\"\"Calculate BLEU score with proper format\"\"\"\n",
    "    if not references or not hypotheses:\n",
    "        return 0.0\n",
    "    \n",
    "    # Convert to proper format for torchtext BLEU\n",
    "    refs_tokenized = [[ref.split()] for ref in references]  # List of [list of tokens]\n",
    "    hyps_tokenized = [hyp.split() for hyp in hypotheses]    # List of tokens\n",
    "    \n",
    "    try:\n",
    "        return bleu_score(hyps_tokenized, refs_tokenized)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def calculate_cer_correct(references, hypotheses):\n",
    "    \"\"\"Calculate Character Error Rate correctly\"\"\"\n",
    "    if not references or not hypotheses:\n",
    "        return 1.0\n",
    "        \n",
    "    total_edits = 0\n",
    "    total_chars = 0\n",
    "    \n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        # Use character-level comparison\n",
    "        total_edits += editdistance.eval(ref, hyp)\n",
    "        total_chars += len(ref)\n",
    "    \n",
    "    return total_edits / total_chars if total_chars > 0 else 1.0\n",
    "\n",
    "def calculate_wer(references, hypotheses):\n",
    "    \"\"\"Calculate Word Error Rate\"\"\"\n",
    "    if not references or not hypotheses:\n",
    "        return 1.0\n",
    "        \n",
    "    total_edits = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        ref_words = ref.split()\n",
    "        hyp_words = hyp.split()\n",
    "        \n",
    "        total_edits += editdistance.eval(ref_words, hyp_words)\n",
    "        total_words += len(ref_words)\n",
    "    \n",
    "    return total_edits / total_words if total_words > 0 else 1.0\n",
    "\n",
    "# =====================================================\n",
    "# 5. UPDATED Evaluation Loop\n",
    "# =====================================================\n",
    "print(\"Starting evaluation...\")\n",
    "\n",
    "references = []  # Ground truth translations\n",
    "hypotheses = []  # Model predictions\n",
    "\n",
    "# ✅ Use your actual tokenized validation files\n",
    "try:\n",
    "    # Read from your tokenized files and decode them\n",
    "    with open(\"/kaggle/working/tokenized/val.src\", \"r\") as f_src, \\\n",
    "         open(\"/kaggle/working/tokenized/val.tgt\", \"r\") as f_trg:\n",
    "\n",
    "        src_lines = f_src.readlines()\n",
    "        trg_lines = f_trg.readlines()\n",
    "        \n",
    "        print(f\"Evaluating {len(src_lines)} sentences...\")\n",
    "        \n",
    "        for i, (src_line, trg_line) in enumerate(zip(src_lines, trg_lines)):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Processed {i} sentences...\")\n",
    "            \n",
    "            # Parse tokenized lines (space-separated integers)\n",
    "            try:\n",
    "                src_tokens = [int(t) for t in src_line.strip().split() if t]\n",
    "                trg_tokens = [int(t) for t in trg_line.strip().split() if t]\n",
    "                \n",
    "                if not src_tokens or not trg_tokens:\n",
    "                    continue\n",
    "                \n",
    "                # Decode tokens to text (removing BOS/EOS)\n",
    "                src_text = sp_ur.decode([t for t in src_tokens if t not in [sp_ur.bos_id(), sp_ur.eos_id()]])\n",
    "                trg_text = sp_ro.decode([t for t in trg_tokens if t not in [sp_ro.bos_id(), sp_ro.eos_id()]])\n",
    "                \n",
    "                # Translate with BEAM SEARCH (improved method)\n",
    "                hypothesis = translate_sentence_improved(src_text, model, sp_ur, sp_ro, \n",
    "                                                       use_beam_search=True, beam_size=5, device=device)\n",
    "                \n",
    "                references.append(trg_text)\n",
    "                hypotheses.append(hypothesis)\n",
    "                \n",
    "                # Print first few examples with comparison\n",
    "                if i < 5:\n",
    "                    # Also get greedy result for comparison\n",
    "                    greedy_result = translate_sentence_improved(src_text, model, sp_ur, sp_ro, \n",
    "                                                              use_beam_search=False, device=device)\n",
    "                    \n",
    "                    print(f\"\\nExample {i+1}:\")\n",
    "                    print(f\"Source:     {src_text}\")\n",
    "                    print(f\"Reference:  {trg_text}\")\n",
    "                    print(f\"Greedy:     {greedy_result}\")\n",
    "                    print(f\"Beam(5):    {hypothesis}\")\n",
    "                    print(f\"Improvement: {'✅' if len(hypothesis.split()) > len(greedy_result.split()) else '🔄'}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sentence {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Limit evaluation to reasonable size for testing\n",
    "            if i >= 1000:  # Evaluate first 1000 sentences\n",
    "                print(f\"Stopping evaluation at {i+1} sentences for efficiency...\")\n",
    "                break\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Make sure tokenized validation files exist\")\n",
    "    exit(1)\n",
    "\n",
    "# =====================================================\n",
    "# 6. Calculate Metrics\n",
    "# =====================================================\n",
    "print(f\"\\nCalculating metrics for {len(references)} sentences...\")\n",
    "\n",
    "if references and hypotheses:\n",
    "    # BLEU Score\n",
    "    try:\n",
    "        bleu = calculate_bleu_correct(references, hypotheses)\n",
    "        print(f\"BLEU Score: {bleu*100:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BLEU: {e}\")\n",
    "\n",
    "    # Character Error Rate\n",
    "    try:\n",
    "        cer = calculate_cer_correct(references, hypotheses)\n",
    "        print(f\"CER: {cer:.4f} ({cer*100:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating CER: {e}\")\n",
    "\n",
    "    # Word Error Rate\n",
    "    try:\n",
    "        wer = calculate_wer(references, hypotheses)\n",
    "        print(f\"WER: {wer:.4f} ({wer*100:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating WER: {e}\")\n",
    "else:\n",
    "    print(\"❌ No valid sentence pairs found for evaluation!\")\n",
    "\n",
    "print(\"\\n✅ Evaluation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8305820,
     "sourceId": 13111767,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
